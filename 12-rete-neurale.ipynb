{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUg8INV78uLzrD0Hcu1qhd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lezione 12: Mettiamo Tutto Insieme - Dal Codice all'Open Day üöÄ\n","\n","Siamo arrivati alla fine del nostro viaggio teorico. Nelle ultime lezioni abbiamo smontato una rete neurale pezzo per pezzo:\n","1.  **L'Anatomia**: Abbiamo visto i \"mattoni\", cio√® i neuroni e gli strati (Lezione 9).\n","2.  **La Prova**: Abbiamo visto come i dati attraversano la rete per produrre una previsione (Lezione 10).\n","3.  **La Misura**: Abbiamo capito come si calcola l'errore di questa previsione (Lezione 10).\n","4.  **La Correzione**: Abbiamo scoperto come la rete usa l'errore per correggere i suoi pesi e migliorare (Lezione 11).\n","\n","Ora, la domanda √®: come si fa tutto questo in pratica? Serve scrivere centinaia di righe di matematica complessa? Fortunatamente, no. Librerie moderne come **TensorFlow (con Keras)** ci permettono di costruire e addestrare reti neurali con una semplicit√† sorprendente."],"metadata":{"id":"iDlERAiWoZEG"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6yQ-r4S7oRE0","executionInfo":{"status":"ok","timestamp":1760041263693,"user_tz":-120,"elapsed":24339,"user":{"displayName":"Francesco Belloni","userId":"11714019492918836159"}},"outputId":"1dd9e045-3349-4784-ed13-38a69127a4fb"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Inizio addestramento della rete neurale...\n","‚úÖ Addestramento completato!\n","\n","Accuratezza della rete neurale sul test set: 99.00%\n"]}],"source":["import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.utils import to_categorical\n","\n","# --- 1. PREPARAZIONE DEI DATI (come abbiamo gi√† imparato) ---\n","df_penguins = sns.load_dataset('penguins').dropna()\n","features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n","target = 'species'\n","X = df_penguins[features]\n","y = df_penguins[target]\n","\n","# Scaliamo le feature numeriche\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Codifichiamo le etichette testuali (le specie) in numeri (0, 1, 2)\n","encoder = LabelEncoder()\n","y_encoded = encoder.fit_transform(y)\n","# E poi in formato \"one-hot\" (es. 'Adelie'=0 diventa [1,0,0]), ideale per le reti neurali\n","y_one_hot = to_categorical(y_encoded)\n","\n","# Dividiamo i dati\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_one_hot, test_size=0.3, random_state=42)\n","\n","\n","# --- 2. COSTRUZIONE DEL MODELLO (L'Anatomia) ---\n","modello = Sequential()\n","# Aggiungiamo uno strato nascosto con 10 neuroni.\n","# input_dim=4 perch√© abbiamo 4 feature. 'relu' √® una funzione di attivazione comune.\n","modello.add(Dense(10, input_dim=4, activation='relu'))\n","# Aggiungiamo lo strato di output con 3 neuroni (uno per ogni specie).\n","# 'softmax' √® l'attivazione perfetta per la classificazione: converte l'output in probabilit√†.\n","modello.add(Dense(3, activation='softmax'))\n","\n","\n","# --- 3. CONFIGURAZIONE DELL'APPRENDIMENTO ---\n","# Qui diciamo alla rete COME imparare.\n","modello.compile(loss='categorical_crossentropy', # La funzione di costo per questo problema\n","                 optimizer='adam',             # Un Gradient Descent avanzato\n","                 metrics=['accuracy'])         # Vogliamo monitorare l'accuratezza\n","\n","\n","# --- 4. ADDESTRAMENTO DEL MODELLO ---\n","print(\"Inizio addestramento della rete neurale...\")\n","# epochs = quante volte la rete vedr√† l'intero training set.\n","# verbose=0 per non stampare l'output di ogni epoca.\n","modello.fit(X_train, y_train, epochs=50, verbose=0)\n","print(\"‚úÖ Addestramento completato!\")\n","\n","\n","# --- 5. VALUTAZIONE ---\n","loss, accuracy = modello.evaluate(X_test, y_test, verbose=0)\n","print(f\"\\nAccuratezza della rete neurale sul test set: {accuracy:.2%}\")"]},{"cell_type":"markdown","source":["Quel blocco di codice, che sembra cos√¨ compatto, sta facendo tutto quello di cui abbiamo parlato nelle ultime quattro lezioni! Vediamo come:\n","\n","* `modello = Sequential([...])` e `modello.add(...)`\n","    > Qui abbiamo costruito l'**Anatomia** della rete, definendo gli strati e i neuroni (Lezione 9).\n","\n","* `loss='categorical_crossentropy'`\n","    > Qui abbiamo scelto l'**Arbitro**, cio√® la Funzione di Costo che misurer√† l'errore dopo ogni previsione (Lezione 10).\n","\n","* `optimizer='adam'`\n","    > Qui abbiamo scelto il **Motore** dell'apprendimento, un algoritmo (come il Gradient Descent) che si occupa di aggiornare i pesi per minimizzare l'errore (Lezione 11).\n","\n","* `modello.fit(...)`\n","    > Questo singolo comando avvia il **Ciclo di Apprendimento**. Per 50 volte (le \"epoche\"), la rete eseguir√† la **Forward Propagation**, calcoler√† l'**Errore** e user√† la **Backpropagation** per correggere i pesi.\n","\n","Le librerie moderne nascondono la complessit√†, ma ora voi sapete esattamente cosa succede dietro le quinte."],"metadata":{"id":"jYvsClECo1ou"}},{"cell_type":"markdown","source":["## Dall'Aula all'Open Day: Teachable Machine\n","\n","Lo strumento che userete durante l'open day, **Teachable Machine**, non √® altro che un'interfaccia grafica bellissima che fa esattamente quello che abbiamo appena fatto nel codice.\n","\n","Quando i ragazzi di 3¬∞ media e i loro genitori vi chiederanno \"ma cosa sta succedendo?\", voi potrete spiegare:\n","\n","| Voi fate questo in Teachable Machine... | E in realt√†, sotto il cofano, succede questo... |\n","| :--- | :--- |\n","| **Caricate le immagini** nelle classi (es. \"Mano Aperta\", \"Pugno Chiuso\"). | State creando il **dataset**, associando i dati (i pixel delle immagini) alle **etichette**. |\n","| **Cliccate il pulsante \"Train\"**. | State eseguendo il comando `model.fit()`. Una potente rete neurale pre-addestrata sta eseguendo decine di **epoche** di **forward** e **backpropagation** per imparare a distinguere le vostre immagini. |\n","| **Vi mettete davanti alla webcam** per il test. | La rete sta eseguendo una velocissima **Forward Propagation** (`model.predict()`) sui fotogrammi della webcam. |\n","| **Vedete le barre di probabilit√†** per ogni classe. | Quello √® il risultato dello strato di output con attivazione **softmax**, che vi dice la \"fiducia\" della rete per ogni possibile classe. |\n","\n","---\n","**Congratulazioni!** Avete completato il percorso teorico. Ora avete una comprensione profonda e intuitiva dei concetti fondamentali del machine learning, che vi permetter√† di presentare il vostro progetto all'open day non come \"magia\", ma come una tecnologia affascinante che ora sapete spiegare."],"metadata":{"id":"B4j5_dvJpCcV"}}]}